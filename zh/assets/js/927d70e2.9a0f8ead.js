"use strict";(self.webpackChunkopenkruise_io=self.webpackChunkopenkruise_io||[]).push([[1045],{2190:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"uniteddeployment","metadata":{"permalink":"/zh/blog/uniteddeployment","editUrl":"https://github.com/openkruise/openkruise.io/tree/master/blog/blog/2019-11-20-uniteddeployment.md","source":"@site/blog/2019-11-20-uniteddeployment.md","title":"UnitedDeploymemt - Supporting Multi-domain Workload Management","description":"Ironically, probably every cloud user knew (or should realized that) failures in Cloud resources","date":"2019-11-20T00:00:00.000Z","formattedDate":"2019\u5e7411\u670820\u65e5","tags":[{"label":"workload","permalink":"/zh/blog/tags/workload"},{"label":"uniteddeployment","permalink":"/zh/blog/tags/uniteddeployment"}],"readingTime":6.005,"truncated":false,"authors":[{"name":"Fei Guo","title":"Maintainer of OpenKruise","url":"https://github.com/Fei-Guo","imageURL":"https://github.com/Fei-Guo.png","key":"Fei-Guo"}],"nextItem":{"title":"Learning Concurrent Reconciling","permalink":"/zh/blog/learning-concurrent-reconciling"}},"content":"Ironically, probably every cloud user knew (or should realized that) failures in Cloud resources\\nare inevitable. Hence, high availability is probably one of the most desirable features that\\nCloud Provider offers for cloud users. For example, in AWS, each geographic region has \\nmultiple isolated locations known as Availability Zones (AZs). \\nAWS provides various AZ-aware solutions to allow the compute or storage resources of the user\\napplications to be distributed across multiple AZs in order to tolerate AZ failure, which indeed\\nhappened in the past. \\n\\nIn Kubernetes, the concept of AZ is not realized by an API object. Instead,\\nan AZ is usually represented by a group of hosts that have the same location label.\\nAlthough hosts within the same AZ can be identified by labels, the capability of distributing Pods across\\nAZs was missing in Kubernetes default scheduler. Hence it was difficult to use single \\n`StatefulSet` or `Deployment` to perform  AZ-aware Pods deployment. Fortunately, \\nin Kubernetes 1.16, a new feature called [\\"Pod Topology Spread Constraints\\"](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/)\\nwas introduced. Users now can add new constraints in the Pod Spec, and scheduler\\nwill enforce the constraints so that Pods can be distributed across failure \\ndomains such as AZs, regions or nodes, in a uniform fashion.\\n\\nIn Kruise, **UnitedDeploymemt** provides an alternative to achieve high availability in\\na cluster that consists of multiple fault domains - that is, managing multiple homogeneous \\nworkloads, and each workload is dedicated to a single `Subset`. Pod distribution across AZs is\\ndetermined by the replica number of each workload.\\nSince each `Subset` is associated with a workload, UnitedDeployment can support\\nfiner-grained rollout and deployment strategies. \\nIn addition, UnitedDeploymemt can be further extended to support\\nmultiple clusters! Let us reveal how UnitedDeployment is designed.\\n\\n\\n## Using `Subsets` to describe domain topology\\n\\nUnitedDeploymemt uses `Subset` to represent a failure domain. `Subset` API\\nprimarily specifies the nodes that forms the domain and the number of replicas, or\\nthe percentage of total replicas, run in this domain. UnitedDeployment manages\\nsubset workloads against a specific domain topology, described by a `Subset` array.\\n\\n```\\ntype Topology struct {\\n\\t// Contains the details of each subset.\\n\\tSubsets []Subset\\n}\\n\\ntype Subset struct {\\n\\t// Indicates the name of this subset, which will be used to generate\\n\\t// subset workload name prefix in the format \'<deployment-name>-<subset-name>-\'.\\n\\tName string\\n\\n\\t// Indicates the node select strategy to form the subset.\\n\\tNodeSelector corev1.NodeSelector\\n\\n\\t// Indicates the number of the subset replicas or percentage of it on the\\n\\t// UnitedDeployment replicas.\\n\\tReplicas *intstr.IntOrString\\n}\\n```\\n\\nThe specification of the subset workload is saved in `Spec.Template`. UnitedDeployment\\nonly supports `StatefulSet` subset workload as of now. An interesting part of `Subset`\\ndesign is that now user can specify **customized Pod distribution** across AZs, which is not\\nnecessarily a uniform distribution in some cases. For example, if the AZ\\nutilization or capacity are not homogeneous, evenly distributing Pods may lead to Pod deployment\\nfailure due to lack of resources. If users have prior knowledge about AZ resource capacity/usage,\\nUnitedDeployment can help to apply an optimal Pod distribution to ensure overall\\ncluster utilization remains balanced. Of course, if not specified, a uniform Pod distribution\\nwill be applied to maximize availability.\\n\\n## Customized subset rollout `Partitions`\\n\\nUser can update all the UnitedDeployment subset workloads by providing a\\nnew version of subset workload template.\\nNote that UnitedDeployment does not control\\nthe entire rollout process of all subset workloads, which is typically done by another rollout\\ncontroller built on top of it. Since the replica number in each `Subset` can be different,\\nit will be much more convenient to allow user to specify the individual rollout `Partition` of each\\nsubset workload instead of using one `Partition` to rule all, so that they can be upgraded in the same pace.\\nUnitedDeployment provides `ManualUpdate` strategy to customize per subset rollout `Partition`.\\n\\n```\\ntype UnitedDeploymentUpdateStrategy struct {\\n\\t// Type of UnitedDeployment update.\\n\\tType UpdateStrategyType\\n\\t// Indicates the partition of each subset.\\n\\tManualUpdate *ManualUpdate\\n}\\n\\ntype ManualUpdate struct {\\n\\t// Indicates number of subset partition.\\n\\tPartitions map[string]int32\\n}\\n```\\n\\n![multi-cluster controller](/img/blog/2019-11-20-uniteddeployment/uniteddeployment-1.png)\\n\\nThis makes it fairly easy to coordinate multiple subsets rollout. For example,\\nas illustrated in Figure 1, assuming UnitedDeployment manages three subsets and\\ntheir replica numbers are 4, 2, 2 respectively, a rollout \\ncontroller can realize a canary release plan of upgrading 50% of Pods in each\\nsubset at a time by setting subset partitions to 2, 1, 1 respectively. \\nThe same cannot be easily achieved by using a single workload controller like `StatefulSet`\\nor `Deployment`.\\n\\n## Multi-Cluster application management (In future)\\n\\nUnitedDeployment can be extended to support multi-cluster workload\\nmanagement. The idea is that `Subsets` may not only\\nreside in one cluster, but also spread over multiple clusters. \\nMore specifically, domain topology specification will associate\\na `ClusterRegistryQuerySpec`, which describes the clusters that UnitedDeployment\\nmay distribute Pods to. Each cluster is represented by a custom resource managed by a\\nClusterRegistry controller using Kubernetes [cluster registry APIs](https://github.com/kubernetes/cluster-registry).\\n\\n```\\ntype Topology struct {\\n  // ClusterRegistryQuerySpec is used to find the all the clusters that\\n  // the workload may be deployed to. \\n  ClusterRegistry *ClusterRegistryQuerySpec\\n  // Contains the details of each subset including the target cluster name and\\n  // the node selector in target cluster.\\n  Subsets []Subset\\n}\\n\\ntype ClusterRegistryQuerySpec struct {\\n  // Namespaces that the cluster objects reside.\\n  // If not specified, default namespace is used.\\n  Namespaces []string\\n  // Selector is the label matcher to find all qualified clusters.\\n  Selector   map[string]string\\n  // Describe the kind and APIversion of the cluster object.\\n  ClusterType metav1.TypeMeta\\n}\\n\\ntype Subset struct {\\n  Name string\\n\\n  // The name of target cluster. The controller will validate that\\n  // the TargetCluster exits based on Topology.ClusterRegistry.\\n  TargetCluster *TargetCluster\\n\\n  // Indicate the node select strategy in the Subset.TargetCluster.\\n  // If Subset.TargetCluster is not set, node selector strategy refers to\\n  // current cluster.\\n  NodeSelector corev1.NodeSelector\\n\\n  Replicas *intstr.IntOrString \\n}\\n\\ntype TargetCluster struct {\\n  // Namespace of the target cluster CRD\\n  Namespace string\\n  // Target cluster name\\n  Name string\\n}\\n```\\n\\nA new `TargetCluster` field is added to the `Subset` API. If it presents, the\\n`NodeSelector` indicates the node selection logic in the target cluster. Now\\nUnitedDeployment controller can distribute application Pods to multiple clusters by\\ninstantiating a `StatefulSet` workload in each target cluster with a specific\\nreplica number (or a percentage of total replica), as illustrated in Figure 2.\\n\\n![multi-cluster\\tcontroller](/img/blog/2019-11-20-uniteddeployment/uniteddeployment-2.png)\\n\\nAt a first glance, UnitedDeployment looks more like a federation\\ncontroller following the design pattern of [Kubefed](https://github.com/kubernetes-sigs/kubefed),\\nbut it isn\'t. The fundamental difference is that Kubefed focuses on propagating arbitrary\\nobject types to remote clusters instead of managing an application across clusters. \\nIn this example, had a Kubefed style controller been used, each `StatefulSet` workload in\\nindividual cluster would have a replica of 100. UnitedDeployment focuses more on\\nproviding the ability of managing multiple workloads in multiple clusters on behalf\\nof one application, which is absent in Kubernetes community to the best of our\\nknowledge.\\n\\n## Summary\\n\\nThis blog post introduces UnitedDeployment, a new controller which helps managing \\napplication spread over multiple domains (in arbitrary clusters). \\nIt not only allows evenly distributing Pods over AZs, \\nwhich arguably can be more efficiently done using the new Pod Topology Spread\\nConstraint APIs though, but also enables flexible workload deployment/rollout and\\nsupports multi-cluster use cases in the future."},{"id":"learning-concurrent-reconciling","metadata":{"permalink":"/zh/blog/learning-concurrent-reconciling","editUrl":"https://github.com/openkruise/openkruise.io/tree/master/blog/blog/2019-11-10-learning-concurrent-reconciling.md","source":"@site/blog/2019-11-10-learning-concurrent-reconciling.md","title":"Learning Concurrent Reconciling","description":"The concept of controller in Kubernete is one of the most important reasons that make it successful.","date":"2019-11-10T00:00:00.000Z","formattedDate":"2019\u5e7411\u670810\u65e5","tags":[{"label":"workload","permalink":"/zh/blog/tags/workload"},{"label":"reconcile","permalink":"/zh/blog/tags/reconcile"},{"label":"controller","permalink":"/zh/blog/tags/controller"}],"readingTime":3.915,"truncated":false,"authors":[{"name":"Fei Guo","title":"Maintainer of OpenKruise","url":"https://github.com/Fei-Guo","imageURL":"https://github.com/Fei-Guo.png","key":"Fei-Guo"}],"prevItem":{"title":"UnitedDeploymemt - Supporting Multi-domain Workload Management","permalink":"/zh/blog/uniteddeployment"},"nextItem":{"title":"Kruise Workload Classification Guidance","permalink":"/zh/blog/workload-classification-guidance"}},"content":"The concept of controller in Kubernete is one of the most important reasons that make it successful.\\nController is the core mechanism that supports Kubernetes APIs to ensure the system reaches \\nthe desired state. By leveraging CRDs/controllers and operators, it is fairly easy for \\nother systems to integrate with Kubernetes. \\n\\nController runtime library and the corresponding controller tool [KubeBuilder](https://book.kubebuilder.io/introduction.html)\\nare widely used by many developers to build their customized Kubernetes controllers. In Kruise project,\\nwe also use Kubebuilder to generate scaffolding codes that implement the \\"reconciling\\" logic. \\nIn this blog post, I will share some learnings from\\nKruise controller development, particularly, about concurrent reconciling. \\n\\nSome people may already notice that controller runtime supports concurrent reconciling.\\nCheck for the options ([source](https://github.com/kubernetes-sigs/controller-runtime/blob/81842d0e78f7111f0566156189806e2801e3adf1/pkg/controller/controller.go#L32))\\nused to create new controller:  \\n\\n```\\ntype Options struct {\\n\\t// MaxConcurrentReconciles is the maximum number of concurrent Reconciles which can be run. Defaults to 1.\\n\\tMaxConcurrentReconciles int\\n\\n\\t// Reconciler reconciles an object\\n\\tReconciler reconcile.Reconciler\\n}\\n```\\n\\nConcurrent reconciling is quite useful when the states of the controller\'s watched objects change so\\nfrequently that a large amount of reconcile requests are sent to and queued in the reconcile queue.\\nMultiple reconcile loops do help drain the reconcile queue much more quickly compared to the default single\\nreconcile loop case. Although this is a great feature for performance, without digging into the code,\\nan immediate concern that a developer may raise is that will this introduce consistency issue? \\ni.e., is it possible that two reconcile loops handle the same object at the same time?\\n\\nThe answer is NO, as you may expect. The \\"magic\\" is enforced by the workqueue\\nimplementation in Kubernetes `client-go`, which is used by controller runtime reconcile queue. \\nThe workqueue algorithm ([source](https://github.com/kubernetes/client-go/blob/a57d0056dbf1d48baaf3cee876c123bea745591f/util/workqueue/queue.go#L65))\\nis demonstrated in Figure 1.\\n\\n![workqueue](/img/blog/2019-11-10-learning-concurrent-reconciling/workqueue.png)\\n\\nBasically, the workqueue uses a `queue` and two `sets` to coordinate the process of handling multiple reconciling \\nrequests against the same object. Figure 1(a) presents the initial state of handling four reconcile requests,\\ntwo of which target the same object A. When a request arrives, the target object is first added to the `dirty set`\\nor dropped if it presents in `dirty set`,  and then pushed to the `queue` only if it is not presented in\\n`processing set`. Figure 1(b) shows the case of adding three requests consecutively. \\nWhen a reconciling loop is ready to serve a request, it gets the target object from the `front` of the queue. The\\nobject is also added to the `processing set` and removed from the `dirty set` (Figure 1(c)).\\nNow if a request of the processing object arrives, the object is only added to the `dirty set`, not\\nto the `queue` (Figure 1(d)). This guarantees that an object is only handled by one reconciling\\nloop. When reconciling is done, the object is removed from the `processing set`. If the object is also\\nshown in the `dirty set`, it is added back to the `back` of the `queue` (Figure 1(e)).\\n\\nThe above algorithm has following implications:\\n* It avoids concurrent reconciling for the same object.\\n* The object processing order can be different from arriving order even if there is only one reconciling thread.\\nThis usually would not be a problem since the controller still reconciles to the final cluster state. However,\\nthe out of order reconciling may cause a significant delay for a request. \\n![workqueue-starve](/img/blog/2019-11-10-learning-concurrent-reconciling/workqueue-starve.png).... For example, as illustrated in \\nFigure 2, assuming there is only one reconciling thread and two requests targeting the same object A arrive, one of\\nthem will be processed and object A will be added to the `dirty set` (Figure 2(b)). \\nIf the reconciling takes a long time and during which a large number of new reconciling requests arrive,\\nthe queue will be filled up by the new requests (Figure 2(c)). When reconciling is done, object A will be\\nadded to the `back` of the `queue` (Figure 2(d)). It would not be handled until all the requests coming after had been\\nhandled, which can cause a noticeable long delay. The workaround is actually simple - **USE CONCURRENT RECONCILES**.\\nSince the cost of an idle go routine is fairly small, the overhead of having multiple reconcile threads is\\nlow even if the controller is idle. It seems that the `MaxConcurrentReconciles` value should\\nbe overwritten to a value larger than the default 1 (CloneSet uses 10 for example).\\n* Last but not the least, reconcile requests can be dropped (if the target exists in `dirty set`). This means\\nthat we cannot assume that the controller can track all the object state change events. Recalling a presentation\\ngiven by [Tim Hockin](https://speakerdeck.com/thockin/edge-vs-level-triggered-logic), Kubernetes controller\\nis level triggered, not edge triggered. It reconciles for state, not for events. \\n\\nThanks for reading the post, hope it helps."},{"id":"workload-classification-guidance","metadata":{"permalink":"/zh/blog/workload-classification-guidance","editUrl":"https://github.com/openkruise/openkruise.io/tree/master/blog/blog/2019-10-10-workload-classification-guidance.md","source":"@site/i18n/zh/docusaurus-plugin-content-blog/2019-10-10-workload-classification-guidance.md","title":"Kruise Workload Classification Guidance","description":"Kubernetes \u76ee\u524d\u5e76\u6ca1\u6709\u4e3a\u4e00\u4e2a\u5e94\u7528\u5e94\u8be5\u4f7f\u7528\u54ea\u4e2a\u63a7\u5236\u5668\u63d0\u4f9b\u660e\u786e\u7684\u6307\u5f15\uff0c\u8fd9\u5c24\u5176\u4e0d\u5229\u4e8e\u7528\u6237\u7406\u89e3\u5e94\u7528\u548c workload \u7684\u5173\u7cfb\u3002","date":"2019-10-10T00:00:00.000Z","formattedDate":"2019\u5e7410\u670810\u65e5","tags":[{"label":"workload","permalink":"/zh/blog/tags/workload"}],"readingTime":6.995,"truncated":false,"authors":[{"name":"Fei Guo","title":"Maintainer of OpenKruise","url":"https://github.com/Fei-Guo","imageURL":"https://github.com/Fei-Guo.png","key":"Fei-Guo"},{"name":"Siyu Wang","title":"Maintainer of OpenKruise","url":"https://github.com/FillZpp","imageURL":"https://github.com/FillZpp.png","key":"FillZpp"}],"prevItem":{"title":"Learning Concurrent Reconciling","permalink":"/zh/blog/learning-concurrent-reconciling"}},"content":"Kubernetes \u76ee\u524d\u5e76\u6ca1\u6709\u4e3a\u4e00\u4e2a\u5e94\u7528\u5e94\u8be5\u4f7f\u7528\u54ea\u4e2a\u63a7\u5236\u5668\u63d0\u4f9b\u660e\u786e\u7684\u6307\u5f15\uff0c\u8fd9\u5c24\u5176\u4e0d\u5229\u4e8e\u7528\u6237\u7406\u89e3\u5e94\u7528\u548c workload \u7684\u5173\u7cfb\u3002\\n\u6bd4\u5982\u8bf4\uff0c\u7528\u6237\u901a\u5e38\u77e5\u9053\u4ec0\u4e48\u65f6\u5019\u5e94\u8be5\u7528 `Job/CronJob` \u6216\u8005 `DaemonSet`\uff0c\u8fd9\u4e9b workload \u7684\u6982\u5ff5\u662f\u975e\u5e38\u660e\u786e\u7684 -- \u524d\u8005\u662f\u4e3a\u4e86\u4efb\u52a1\u7c7b\u578b\u7684\u5e94\u7528\u90e8\u7f72\u3001\u540e\u8005\u5219\u662f\u9762\u5411\u9700\u8981\u5206\u53d1\u5230\u6bcf\u4e2a node \u4e0a\u7684\u957f\u671f\u8fd0\u884c Pod\u3002\\n\\n\u4f46\u662f\u53e6\u4e00\u4e9b workload \u6bd4\u5982 `Deployment` \u548c `StatefulSet` \u4e4b\u95f4\u7684\u754c\u9650\u662f\u6bd4\u8f83\u6a21\u7cca\u7684\u3002\u4e00\u4e2a\u901a\u8fc7 `Deployment` \u90e8\u7f72\u7684\u5e94\u7528\u4e5f\u53ef\u4ee5\u901a\u8fc7 `StatefulSet` \u90e8\u7f72\uff0c`StatefulSet` \u5bf9 Pod \u7684 `OrderedReady` \u7b56\u7565\u5e76\u975e\u662f\u5f3a\u5236\u7684\u3002\u800c\u4e14\uff0c\u968f\u7740 Kubernetes \u793e\u533a\u4e2d\u8d8a\u6765\u8d8a\u591a\u7684\u81ea\u5b9a\u4e49 controllers/operators \u53d8\u7684\u6210\u719f\uff0c\u7528\u6237\u5c31\u8d8a\u96be\u4ee5\u4e3a\u81ea\u5df1\u7684\u5e94\u7528\u627e\u5230\u4e00\u4e2a\u6700\u5408\u9002\u7684 workload \u6765\u7ba1\u7406\uff0c\u5c24\u5176\u662f\u4e00\u4e9b\u63a7\u5236\u5668\u7684\u529f\u80fd\u4e0a\u90fd\u5b58\u5728\u91cd\u5408\u90e8\u5206\u3002\\n\\nKruise \u5c1d\u8bd5\u5728\u4e24\u4e2a\u65b9\u9762\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff1a\\n\\n- \u5728 Kruise \u4e2d\u8c28\u614e\u8bbe\u8ba1\u65b0\u7684\u63a7\u5236\u5668\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u529f\u80fd\u91cd\u590d\u7ed9\u7528\u6237\u6765\u5e26\u56f0\u6270\\n- \u4e3a\u6240\u6709\u63d0\u4f9b\u51fa\u6765\u7684 workload \u63a7\u5236\u5668\u521b\u5efa\u4e00\u4e2a\u5206\u7c7b\u673a\u5236\uff0c\u65b9\u4fbf\u7528\u6237\u66f4\u5bb9\u6613\u7406\u89e3\u5b83\u4eec\u7684\u4f7f\u7528\u573a\u666f\u3002\u6211\u4eec\u4e0b\u9762\u4f1a\u8be6\u7ec6\u63cf\u8ff0\u4e00\u4e0b\uff0c\u9996\u5148\u662f controller \u547d\u540d\u4e0a\u7684\u89c4\u8303\uff1a\\n\\n### Controller \u547d\u540d\u60ef\u4f8b\\n\\n\u4e00\u4e2a\u6613\u4e8e\u7406\u89e3\u7684 controller \u540d\u5b57\u5bf9\u4e8e\u7528\u6237\u9009\u7528\u662f\u975e\u5e38\u6709\u5e2e\u52a9\u7684\u3002\u7ecf\u8fc7\u5bf9\u5185\u5916\u90e8\u4e0d\u5c11 Kubernetes \u7528\u6237\u7684\u54a8\u8be2\uff0c\u6211\u4eec\u51b3\u5b9a\u5728 Kruise \u4e2d\u5b9e\u884c\u4ee5\u4e0b\u7684\u547d\u540d\u60ef\u4f8b\uff08\u8fd9\u4e9b\u60ef\u4f8b\u4e0e\u76ee\u524d\u4e0a\u6e38\u7684 controller \u547d\u540d\u5e76\u4e0d\u51b2\u7a81\uff09\uff1a\\n\\n- **Set** \u540e\u7f00\uff1a\u8fd9\u7c7b controller \u4f1a\u76f4\u63a5\u64cd\u4f5c\u548c\u7ba1\u7406 Pod\uff0c\u6bd4\u5982 `CloneSet`, `ReplicaSet`, `SidecarSet` \u7b49\u3002\u5b83\u4eec\u63d0\u4f9b\u4e86 Pod \u7ef4\u5ea6\u7684\u591a\u79cd\u90e8\u7f72\u3001\u53d1\u5e03\u7b56\u7565\u3002\\n- **Deployment** \u540e\u7f00\uff1a\u8fd9\u7c7b controller \u4e0d\u4f1a\u76f4\u63a5\u5730\u64cd\u4f5c Pod\uff0c\u5b83\u4eec\u901a\u8fc7\u64cd\u4f5c\u4e00\u4e2a\u6216\u591a\u4e2a **Set** \u7c7b\u578b\u7684 workload \u6765\u95f4\u63a5\u7ba1\u7406 Pod\uff0c\u6bd4\u5982 `Deployment` \u7ba1\u7406 `ReplicaSet` \u6765\u63d0\u4f9b\u4e00\u4e9b\u989d\u5916\u7684\u6eda\u52a8\u7b56\u7565\uff0c\u4ee5\u53ca `UnitedDeployment` \u652f\u6301\u7ba1\u7406\u591a\u4e2a `StatefulSet`/`AdvancedStatefulSet` \u6765\u5c06\u5e94\u7528\u90e8\u7f72\u5230\u4e0d\u540c\u7684\u53ef\u7528\u533a\u3002\\n- **Job** \u540e\u7f00\uff1a\u8fd9\u7c7b controller \u4e3b\u8981\u7ba1\u7406\u77ed\u671f\u6267\u884c\u7684\u4efb\u52a1\uff0c\u6bd4\u5982 `BroadcastJob` \u652f\u6301\u5c06\u4efb\u52a1\u7c7b\u578b\u7684 Pod \u5206\u53d1\u5230\u96c6\u7fa4\u4e2d\u6240\u6709 Node \u4e0a\u3002\\n\\n**Set**, **Deployment** \u548c **Job** \u90fd\u662f\u88ab Kubernetes \u793e\u533a\u5e7f\u6cdb\u63a5\u53d7\u7684\u6982\u5ff5\uff0c\u5728 Kruise \u4e2d\u7ed9\u4ed6\u4eec\u5b9a\u4e49\u4e86\u660e\u786e\u7684\u6269\u5c55\u89c4\u8303\u3002\\n\\n\u6211\u4eec\u80fd\u5426\u5bf9\u6709\u76f8\u540c\u540e\u7f00\u7684 controller \u505a\u8fdb\u4e00\u6b65\u533a\u5206\u5462\uff1f\u901a\u5e38\u6765\u8bf4\u524d\u7f00\u524d\u9762\u7684\u540d\u5b57\u5e94\u8be5\u662f\u8ba9\u4eba\u80fd\u4e00\u76ee\u4e86\u7136\u7684\uff0c\u4e0d\u8fc7\u4e5f\u6709\u4e00\u4e9b\u60c5\u51b5\u4e0b\u5f88\u96be\u4e00\u8bed\u63cf\u8ff0 controller \u81ea\u8eab\u7684\u884c\u4e3a\u3002\u53ef\u4ee5\u770b\u4e00\u4e0b `StatefulSet` \u6765\u6e90\u7684\u8fd9\u4e2a [issue](https://github.com/kubernetes/kubernetes/issues/27430)\uff0c\u793e\u533a\u7528\u4e86\u56db\u4e2a\u6708\u7684\u65f6\u95f4\u624d\u51b3\u5b9a\u7528 `StatefulSet` \u8fd9\u4e2a\u540d\u5b57\u4ee3\u66ff\u8fc7\u53bb\u7684 `PetSet`\uff0c\u5c3d\u7ba1\u65b0\u540d\u5b57\u4e5f\u8ba9\u4eba\u770b\u8d77\u6765\u6bd4\u8f83\u56f0\u60d1\u3002\\n\\n\u8fd9\u4e2a\u4f8b\u5b50\u8bf4\u660e\u4e86\u6709\u65f6\u5019\u4e00\u4e2a\u7cbe\u5fc3\u8ba1\u5212\u7684\u540d\u5b57\u4e5f\u4e0d\u4e00\u5b9a\u6709\u52a9\u4e8e\u6807\u8bc6\u8fd9\u4e2a controller\u3002\u56e0\u6b64\uff0cKruise \u5e76\u4e0d\u6253\u7b97\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u800c\u662f\u901a\u8fc7\u4ee5\u4e0b\u7684\u6807\u51c6\u6765\u5e2e\u52a9\u5bf9 **Set** \u7c7b\u578b\u7684 controller \u5206\u7c7b\u3002\\n\\n### \u56fa\u5b9a Pod \u540d\u5b57\\n\\n`StatefulSet` \u7684\u4e00\u4e2a\u72ec\u6709\u7684\u7279\u6027\u662f\u652f\u6301\u4e00\u81f4\u7684 Pod \u7f51\u7edc\u548c\u5b58\u50a8\u6807\u8bc6\uff0c\u8fd9\u5728\u672c\u8d28\u4e0a\u662f\u901a\u8fc7\u56fa\u5b9a Pod \u540d\u5b57\u6765\u5b9e\u73b0\u7684\u3002Pod \u540d\u5b57\u53ef\u4ee5\u7528\u4e8e\u6807\u8bc6\u7f51\u7edc\u548c\u5b58\u50a8\uff0c\u56e0\u4e3a\u5b83\u662f DNS record \u7684\u4e00\u90e8\u5206\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a PVC \u7684\u540d\u5b57\u3002\u65e2\u7136 `StatefulSet` \u4e0b\u7684 Pod \u90fd\u662f\u901a\u8fc7\u540c\u4e00\u4e2a\u6a21\u677f\u521b\u5efa\u51fa\u6765\u7684\uff0c\u4e3a\u4ec0\u4e48\u9700\u8981\u8fd9\u4e2a\u7279\u6027\u5462\uff1f\u4e00\u4e2a\u5e38\u89c1\u7684\u4f8b\u5b50\u5c31\u662f\u7528\u4e8e\u7ba1\u7406\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u670d\u52a1\uff0c\u6bd4\u5982 etcd \u6216 Zookeeper\u3002\u8fd9\u7c7b\u5e94\u7528\u9700\u8981\u77e5\u9053\u96c6\u7fa4\u6784\u6210\u7684\u6240\u6709\u6210\u5458\uff0c\u5e76\u4e14\u5728\u91cd\u5efa\u3001\u53d1\u5e03\u540e\u90fd\u9700\u8981\u4fdd\u6301\u539f\u6709\u7684\u7f51\u7edc\u6807\u8bc6\u548c\u78c1\u76d8\u6570\u636e\u3002\u800c\u50cf `ReplicaSet`, `DaemonSet` \u8fd9\u7c7b\u7684\u63a7\u5236\u5668\u662f\u9762\u5411\u65e0\u72b6\u6001\u7684\uff0c\u5b83\u4eec\u5e76\u4e0d\u4f1a\u65b0\u5efa Pod \u65f6\u5e76\u4e0d\u4f1a\u590d\u7528\u8fc7\u53bb\u7684 Pod \u540d\u5b57\u3002\\n\\n\u4e3a\u4e86\u652f\u6301\u6709\u72b6\u6001\uff0c\u63a7\u5236\u5668\u7684\u5b9e\u73b0\u4e0a\u4f1a\u6bd4\u8f83\u56fa\u5b9a\u3002`StatefulSet` \u4f9d\u8d56\u4e8e\u7ed9\u6bcf\u4e2a Pod \u540d\u5b57\u4e2d\u52a0\u5165\u4e00\u4e2a\u5e8f\u53f7\uff0c\u5728\u6269\u7f29\u5bb9\u548c\u6eda\u52a8\u5347\u7ea7\u7684\u65f6\u5019\u90fd\u9700\u8981\u6309\u7167\u8fd9\u4e2a\u5e8f\u53f7\u7684\u987a\u5e8f\u6765\u6267\u884c\u3002\u4f46\u8fd9\u6837\u4e00\u6765\uff0c`StatefulSet` \u4e5f\u5c31\u65e0\u6cd5\u505a\u5230\u53e6\u4e00\u4e9b\u589e\u5f3a\u529f\u80fd\uff0c\u6bd4\u5982\uff1a\\n\\n- \u5f53\u7f29\u5c0f replicas \u65f6\u9009\u62e9\u7279\u5b9a\u7684 Pod \u6765\u5220\u9664\uff0c\u8fd9\u4e2a\u529f\u80fd\u5728\u8de8\u591a\u4e2a\u53ef\u7528\u533a\u90e8\u7f72\u7684\u65f6\u5019\u4f1a\u7528\u5230\u3002\\n- \u628a\u4e00\u4e2a\u5b58\u91cf\u7684 Pod \u63a5\u7ba1\u5230\u53e6\u4e00\u4e2a workload \u4e0b\u9762\uff08\u6bd4\u5982 `StatefulSet`\uff09\\n\\n\u6211\u4eec\u53d1\u73b0\u5f88\u591a\u4e91\u539f\u751f\u5e94\u7528\u5e76\u4e0d\u9700\u8981\u8fd9\u4e2a\u6709\u72b6\u6001\u7684\u7279\u6027\u6765\u56fa\u5b9a Pod \u540d\u5b57\uff0c\u800c `StatefulSet` \u53c8\u5f88\u96be\u5728\u5176\u4ed6\u65b9\u9762\u505a\u6269\u5c55\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0cKruise \u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u63a7\u5236\u5668 `CloneSet` \u6765\u7ba1\u7406\u65e0\u72b6\u6001\u5e94\u7528\uff0c`CloneSet` \u63d0\u4f9b\u4e86\u5bf9 PVC \u6a21\u677f\u7684\u652f\u6301\uff0c\u5e76\u4e14\u4e3a\u5e94\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u53ef\u9009\u7b56\u7565\u3002\u4ee5\u4e0b\u8868\u4e2d\u6bd4\u8f83\u4e86 Advanced StatefulSet \u548c CloneSet \u4e00\u4e9b\u65b9\u9762\u7684\u80fd\u529b\uff1a\\n\\n| Features   |     Advanced StatefulSet      |  CloneSet |\\n|----------|:-------------:|:------:|\\n| PVC | Yes | Yes |\\n| Pod name | Ordered | Random |\\n| Inplace upgrade | Yes | Yes |\\n| Max unavailable | Yes | Yes |\\n| Selective deletion | No | Yes |\\n| Selective upgrade | No | Yes |\\n| Change Pod ownership | No | Yes |\\n\\n\u76ee\u524d\u5bf9\u4e8e Kruise \u7528\u6237\u7684\u5efa\u8bae\u662f\uff0c\u5982\u679c\u4f60\u7684\u5e94\u7528\u9700\u8981\u56fa\u5b9a\u7684 Pod \u540d\u5b57\uff08\u7f51\u7edc\u548c\u5b58\u50a8\u6807\u8bc6\uff09\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 `Advanced StatefulSet`\uff0c\u5426\u5219 `CloneSet` \u5e94\u8be5\u662f **Set** \u7c7b\u578b\u63a7\u5236\u5668\u7684\u9996\u9009\u3002\\n\\n### \u603b\u7ed3\\n\\nKruise \u4f1a\u4e3a\u5404\u79cd workload \u9009\u62e9\u660e\u786e\u7684\u540d\u5b57\uff0c\u672c\u6587\u76ee\u6807\u662f\u80fd\u4e3a Kruise \u7528\u6237\u63d0\u4f9b\u9009\u62e9\u6b63\u786e controller \u90e8\u7f72\u5e94\u7528\u7684\u6307\u5f15\u3002\\n\u5e0c\u671b\u5bf9\u4f60\u6709\u5e2e\u52a9\uff01"}]}')}}]);